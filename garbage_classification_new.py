# -*- coding: utf-8 -*-
"""Garbage_classification_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15HqKHuJWL7ipFVu3kT-Xz5FKVZvHGq0q
"""

import keras  
import numpy as np
import matplotlib.pyplot as plt 
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img 
from keras.layers import Conv2D, Flatten, MaxPooling2D, Dense ,Dropout
from keras.models import Sequential
from keras.applications.vgg16 import VGG16
from keras import models
from keras.optimizers import Adagrad,RMSprop, SGD, Adam, Nadam
from keras.callbacks import EarlyStopping, ModelCheckpoint

from google.colab import files
uploaded = files.upload()

import os
import zipfile

# 將下載到的資料集解壓縮至/tmp
files=zipfile.ZipFile("archive (1).zip")     
files.extractall('/tmp')
files.close()

print(os.listdir('/tmp/'))

print(os.listdir('/tmp/garbage classification/Garbage classification/'))

print(os.listdir('/tmp/garbage classification/'))

base_path = '/tmp/garbage classification/Garbage classification'

img_list = glob.glob(os.path.join(base_path, '*/*.jpg')) #將在base_path中且命名為'....jpg'的檔案都抓到img_list

print(len(img_list))

for i, img_path in enumerate(random.sample(img_list, 6)):
    img = load_img(img_path)
    img = img_to_array(img, dtype=np.uint8)

    plt.subplot(2, 3, i+1)
    plt.imshow(img.squeeze())

'''增加了這格'''
INPUT = '/tmp/'
GLASS = INPUT + 'garbage classification/Garbage classification/glass/'
PAPER = INPUT + 'garbage classification/Garbage classification/paper/'
CARDBOARD = INPUT + 'garbage classification/Garbage classification/cardboard/'
PLASTIC = INPUT + 'garbage classification/Garbage classification/plastic/'
METAL = INPUT + 'garbage classification/Garbage classification/metal/'
TRASH = INPUT + 'garbage classification/Garbage classification/trash/'
MATERIALS = [GLASS, PAPER, CARDBOARD, PLASTIC, METAL, TRASH]

'''增加了這格'''
def load_csv(subset):
    
    if subset == 'train':
        fname = INPUT + 'one-indexed-files-notrash_train.txt'#one-indexed-files-notrash_test.txt
    elif subset == 'validation':
        fname = INPUT + 'one-indexed-files-notrash_val.txt'
    elif subset == 'test':
        fname = INPUT + 'one-indexed-files-notrash_test.txt'
    else:
        raise ValueError('subset must be "train", "validation" or "test"')
    
    df = pd.read_csv(fname, sep=' ', names=['file', 'label'])
    
    # change the labels to be from 0 to 5
    df['label'] -= 1
    
    # create path column
    df['folder'] = [MATERIALS[i] for i in df['label']]
    df['path'] = df['folder'] + df['file']
    
    # change type of label for datagenerator
    df['label_str'] = df['label'].astype(str)
    
    return df

'''增加了這格'''
df_train = load_csv('train')
df_valid = load_csv('validation')
df_test = load_csv('test')

df_train.head()

img_shape = (224, 224, 3) # default values

train_batch_size = 128
val_batch_size = 32

train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.1,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    vertical_flip=True,
    validation_split=0.1
)

test_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.1
)

train_generator = train_datagen.flow_from_dataframe(
    df_train,  #base_path,                   '''base_path改成df_train'''
    x_col='path',
    y_col='label_str',
    target_size = (img_shape[0], img_shape[1]),
    batch_size = train_batch_size,
    class_mode = 'categorical',
    #subset='training',                     '''這裡有刪'''
    #seed=0                           '''這裡有刪'''
)

validation_generator = test_datagen.flow_from_dataframe(
    df_valid,  #base_path,                  '''base_path改成df_valid'''
    x_col='path',
    y_col='label_str',
    target_size = (img_shape[0], img_shape[1]),
    batch_size = val_batch_size,
    class_mode = 'categorical',
    shuffle=False,
    #subset='validation',                   '''這裡有刪'''
    #seed=0                          '''這裡有刪'''
)
test_generator = test_datagen.flow_from_dataframe(
    df_test, #base_path,                   '''base_path改成df_test'''
    x_col='path',
    y_col='label_str',
    target_size = (img_shape[0], img_shape[1]),
    batch_size = val_batch_size,
    class_mode = 'categorical',
    shuffle=False,)

labels = (train_generator.class_indices)
labels = dict((v,k) for k,v in labels.items())

print(labels)

vgg = VGG16(weights = 'imagenet',
              include_top = False,
              input_shape = img_shape)
# Freeze the layers except the last 3 layers
for layer in vgg.layers[:-3]:
    layer.trainable = False

# Create the model
model = Sequential()
 
# Add the mobilenet convolutional base model
model.add(vgg)
# Add new layers
model.add(Flatten())
# model.add(Dense(1024, activation='relu'))
# model.add(Dropout(0.2))
model.add(Dense(6, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy',
              optimizer=Nadam(lr=1e-4),
              metrics=['acc'])

model.summary()

# Train the model
es = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)]
mc = ModelCheckpoint('VGG16 Garbage Classifier.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples/train_generator.batch_size ,
    epochs=30,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples/validation_generator.batch_size,
    verbose=0,
    callbacks = [es, mc])

test_x, test_y = validation_generator.__getitem__(7)

preds = model.predict(test_x)

plt.figure(figsize=(16, 16))                      '''有新增if else 不過無傷大雅'''
for i in range(16):
    plt.subplot(4, 4, i+1)
    pred = ''
    if labels[np.argmax(preds[i])] == '0':
      pred = 'glass'
    elif labels[np.argmax(preds[i])] == '1':
      pred = 'paper'
    elif labels[np.argmax(preds[i])] == '2':
      pred = 'cardboard'
    elif labels[np.argmax(preds[i])] == '3':
      pred = 'plastic'
    elif labels[np.argmax(preds[i])] == '4':
      pred = 'metal'
    elif labels[np.argmax(preds[i])] == '5':
      pred = 'trash'

    truth = ''
    if labels[np.argmax(test_y[i])] == '0':
      truth = 'glass'
    elif labels[np.argmax(test_y[i])] == '1':
      truth = 'paper'
    elif labels[np.argmax(test_y[i])] == '2':
      truth = 'cardboard'
    elif labels[np.argmax(test_y[i])] == '3':
      truth = 'plastic'
    elif labels[np.argmax(test_y[i])] == '4':
      truth = 'metal'
    elif labels[np.argmax(test_y[i])] == '5':
      truth = 'trash'
    
    plt.title('pred:%s / truth:%s' % (pred, truth))
    plt.imshow(test_x[i])


score = model.evaluate(test_x,test_y,batch_size=10, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epoch = range(len(acc))

plt.plot(epoch, acc, 'b', label='Training acc')
plt.plot(epoch, val_acc, 'r', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epoch, loss, 'b', label='Training loss')
plt.plot(epoch, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

from google.colab import files
uploaded = files.upload()

from keras.preprocessing.image import img_to_array
from keras.applications.imagenet_utils import preprocess_input
from PIL import Image
import numpy as np

img = load_img('metal.jpg', target_size=(224, 224, 3))
plt.imshow(img)
img = img_to_array(img)

img = np.expand_dims(img, axis=0)
img = preprocess_input(img)

preds = model.predict(img)
pred = ''
if labels[np.argmax(preds)] == '0':                  '''有新增if else 不過無傷大雅'''
  pred = 'glass'
elif labels[np.argmax(preds)] == '1':
  pred = 'paper'
elif labels[np.argmax(preds)] == '2':
  pred = 'cardboard'
elif labels[np.argmax(preds)] == '3':
  pred = 'plastic'
elif labels[np.argmax(preds)] == '4':
  pred = 'metal'
elif labels[np.argmax(preds)] == '5':
  pred = 'trash'
plt.title('pred:%s' % (pred))

'''新增了這格'''
from sklearn.metrics import confusion_matrix, f1_score
pred_valid = np.argmax(model.predict(validation_generator), axis=1)
pred_test = np.argmax(model.predict(test_generator), axis=1)

score_valid = f1_score(df_valid['label'], pred_valid, average='macro')
score_test = f1_score(df_test['label'], pred_test, average='macro')

'''新增了這格'''
def plot_cm(y_true, y_pred, title, classes):
    '''
    Fancy confusion matrix plot.
    '''
    y_pred = y_pred.astype(int)
    
    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))
    cm_sum = np.sum(cm, axis=1, keepdims=True)
    cm_perc = cm / cm_sum.astype(float) * 100
    
    annot = np.empty_like(cm).astype(str)
    nrows, ncols = cm.shape
    
    for i in range(nrows):
        for j in range(ncols):
            c = cm[i, j]
            p = cm_perc[i, j]
            if i == j:
                s = cm_sum[i]
                annot[i, j] = '%.1f%%\n%d/%d' % (p, c, s)
            elif c == 0:
                annot[i, j] = ''
            else:
                annot[i, j] = '%.1f%%\n%d' % (p, c)
    
    cm = pd.DataFrame(cm, index=classes, columns=classes)
    cm.index.name = 'Actual'
    cm.columns.name = 'Predicted'
    
    fig, ax = plt.subplots()
    plt.title(title)
    sns.heatmap(cm, cmap='viridis', annot=annot, fmt='', ax=ax)

'''新增了這格'''
classes = ['glass', 'paper', 'cardboard', 'plastic', 'metal', 'trash']
plot_cm(df_valid['label'], pred_valid, 'Validation\nf1_score=' + str('%.3f' %score_valid), classes)

'''新增了這格'''
plot_cm(df_test['label'], pred_test, 'Test\nf1_score=' + str('%.3f' %score_test), classes)

"""# 新增區段"""